{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Exploring Patterns of Population Structure and Environmental Associations to Aridity Across the Range of Loblolly Pine\n",
    "\n",
    "##Introduction\n",
    "\n",
    "In this set of analyses, we will be making use of data from the Eckert et al. 2010 paper to explore patterns of phenotypic and environmental associations among populations of loblolly pine.\n",
    "\n",
    "\n",
    "###Abstract\n",
    "\n",
    "Natural populations of forest trees exhibit striking phenotypic adaptations to diverse environmental\n",
    "gradients, thereby making them appealing subjects for the study of genes underlying ecologically relevant phenotypes. Here, we use a genome-wide data set of single nucleotide polymorphisms genotyped across 3059 functional genes to study patterns of population structure and identify loci associated with aridity across the natural range of loblolly pine (Pinus taeda L.). Overall patterns of population structure, as inferred using principal components and Bayesian cluster analyses, were consistent with three genetic clusters likely resulting from expansions out of Pleistocene refugia located in Mexico and Florida. A novel application of association analysis, which removes the confounding effects of shared ancestry on correlations between genetic and environmental variation, identified five loci correlated with aridity. These loci were primarily involved with abiotic stress response to temperature and drought. A unique set of 24 loci was identified as FST outliers on the basis of the genetic clusters identified previously and after accounting for expansions out of Pleistocene refugia. These loci were involved with a diversity of physiological processes. Identification of nonoverlapping sets of loci highlights the fundamental differences implicit in the use of either method and suggests a pluralistic, yet complementary, approach to the identification of genes underlying ecologically relevant phenotypes.\n",
    "\n",
    "\n",
    "##Overview of tasks\n",
    "\n",
    "In general, what you will be doing is working your way from loading and saving data related to this study, to corrections for population structure, to looking for associations between genotypes and phenotypes, genotypes and the environment (`Bayenv2`), and genotypes+phenotypes+environment (`SQUAT`)\n",
    "\n",
    "## This notebook\n",
    "\n",
    "This notebook will take you through executing Principal Components Analysis on the data, checking for outliers (and fixing them) and determining how many PCA axes to use for downstream correction of population structure.\n",
    "\n",
    "You will:\n",
    "\n",
    "1. Interface between Python and R to execute some commands\n",
    "1. Get some experience with plotting in matplotlib\n",
    "1. Create and save PCA axes\n",
    "1. Learn about applying commands to `DataFrames`, similar to `R`'s (`apply`, `sapply`, `mapply`, etc)\n",
    "\n",
    "As with the previous notebook, execute the cell with the imports and continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from IPython.display import Image\n",
    "import pandas as pd\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import rpy2\n",
    "from rpy2 import robjects as ro\n",
    "import pandas.rpy.common as com\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import operator\n",
    "import scipy as sp\n",
    "import traceback\n",
    "from sklearn import preprocessing\n",
    "from IPython.parallel import Client\n",
    "from subprocess import Popen, PIPE\n",
    "import shutil\n",
    "from IPython.display import FileLink, FileLinks, Image\n",
    "import psutil\n",
    "import multiprocessing\n",
    "from hdfstorehelper import HDFStoreHelper\n",
    "import warnings\n",
    "import pandas\n",
    "import dill\n",
    "warnings.simplefilter(\"ignore\", pandas.io.pytables.PerformanceWarning)\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext rpy2.ipython\n",
    "pd.set_option('display.width', 80)\n",
    "pd.set_option('max.columns', 30)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Set up some functions again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_homozygous(gt):\n",
    "    if len(set([x.strip() for x in gt.split(\"/\")])) == 1:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def get_allele_counts(counts):\n",
    "    a = {}\n",
    "    het = 0\n",
    "    for gt in counts.index:\n",
    "        for allele in [x.strip() for x in gt.split(\"/\")]:\n",
    "            if not allele in a:\n",
    "                a[allele] = 0\n",
    "            a[allele] += counts[gt]\n",
    "        if not is_homozygous(gt):\n",
    "            het += counts[gt]\n",
    "    return sorted(a.items(), key=lambda x: x[1], reverse=True), het\n",
    "\n",
    "\n",
    "def get_correction(n):\n",
    "    #for finite sample size\n",
    "    return (2*n)/(2*n-1)\n",
    "\n",
    "\n",
    "def get_allele_freqs(locus):\n",
    "    locus = locus[locus != '?/?']\n",
    "    locus = locus[locus != 'NA']\n",
    "    c = locus.value_counts()\n",
    "    c = c.sort(inplace=False, ascending=False)\n",
    "    allele_counts = get_allele_counts(c)\n",
    "    total_alleles = 2.0*sum(c)\n",
    "    num_individuals = sum(c)\n",
    "    A = \"\"\n",
    "    a = \"\"\n",
    "    P = 0\n",
    "    Q = 0\n",
    "    if len(allele_counts[0]) == 2:\n",
    "        A = allele_counts[0][0][0]\n",
    "        a = allele_counts[0][1][0]\n",
    "        P = allele_counts[0][0][1]\n",
    "        Q = allele_counts[0][1][1]\n",
    "    else:\n",
    "        A = allele_counts[0][0][0]\n",
    "        P = P = allele_counts[0][0][1]\n",
    "    PQ = allele_counts[-1]\n",
    "    p = P/total_alleles\n",
    "    q = Q/total_alleles\n",
    "    assert p + q == 1.0\n",
    "    He = 2 * p * q * get_correction(num_individuals)\n",
    "    Ho = PQ*1.0/num_individuals\n",
    "    Fis = 1 - (Ho/He)\n",
    "    #print p, q, He, Ho, Fis\n",
    "    ret = pd.Series({\"p\":p, \n",
    "                      \"q\":q,\n",
    "                      \"P\":P,\n",
    "                      \"Q\":Q,\n",
    "                      \"He\":He,\n",
    "                      \"Ho\":Ho, \n",
    "                      \"Fis\":Fis,\n",
    "                    \"PQ\": PQ,\n",
    "                    \"total_alleles\":total_alleles,\n",
    "                    \"num_indiv\":num_individuals,\n",
    "                    \"A\":A,\n",
    "                    \"a\":a})\n",
    "    return ret\n",
    "\n",
    "\n",
    "def plot_hist(df, index):\n",
    "    d = df.ix[index,:]\n",
    "    plt.hist(d, bins=20)\n",
    "    plt.title(\"%s %.2f $\\pm$ %.3f [%.2f, %.2f]\" % (index, \n",
    "                                                   np.mean(d), \n",
    "                                                   np.std(d),\n",
    "                                                  np.min(d),\n",
    "                                                  np.max(d)))\n",
    "    \n",
    "\n",
    "def convert_to_z12(locus):\n",
    "    freq = af[locus.name]\n",
    "    trans = {\"%s/%s\" % (freq[\"A\"],freq[\"A\"]): 0,\n",
    "            \"%s/%s\" % (freq[\"a\"],freq[\"a\"]): 2,\n",
    "            \"%s/%s\" % (freq[\"A\"],freq[\"a\"]): 1,\n",
    "            \"%s/%s\" % (freq[\"a\"],freq[\"A\"]): 1,\n",
    "            \"?/?\":-1}\n",
    "    return locus.apply(lambda x: trans[x])\n",
    "\n",
    "\n",
    "def center_and_standardize_value(val, u, var):\n",
    "    if val == -1:\n",
    "        return 0.0\n",
    "    return (val-u)/np.sqrt(var)\n",
    "\n",
    "\n",
    "def center_and_standardize(snp):\n",
    "    maf = af.ix[\"q\",snp.name]\n",
    "    u = np.mean([x for x in snp if x != -1])\n",
    "    var = np.sqrt(maf*(1-maf))\n",
    "    return snp.apply(center_and_standardize_value, args=(u, var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdf = HDFStoreHelper(\"data.hd5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####The cell below sets up the interface between Pythong and R.  You'll see.  It's slick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = ro.r\n",
    "prcomp = r('prcomp')\n",
    "summary = r('summary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####The following cells will perform:\n",
    "\n",
    "1. retrieval of the normalized genotypes for PCA\n",
    "1. Sending that data to R, running `prcomp` and saving the results back in Python\n",
    "1. Extracting individual component data from the `prcomp_res` object.\n",
    "1. Plotting PC1 vs PC2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca_std = hdf.get('pca_std')\n",
    "prcomp_res = prcomp(pca_std, scale=False, center=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(summary(prcomp_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = com.convert_robj(prcomp_res.rx2(\"x\"))\n",
    "x.index = pca_std.index\n",
    "x.ix[0:5,0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(x.PC1, x.PC2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Notice anything about this plot?\n",
    "\n",
    "* Does it look like you would expect a PCA plot to look like?\n",
    "* In the paper, they do end up making some corrections for outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Just for fun, let's see how many axes might describe the population structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "source(\"tw_calc.R\")\n",
    "test=read.table(\"twtable\", header=F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TWcalc = r('TWcalc')\n",
    "tw = TWcalc(com.convert_to_r_matrix(pca_std), 25)\n",
    "tw_p = com.convert_robj(tw.rx2(2))\n",
    "tw_e = com.convert_robj(tw.rx2(1))\n",
    "tw_num = 0\n",
    "for i, p in enumerate(tw_p):\n",
    "    print i, p\n",
    "    if p > 0.05:\n",
    "        tw_num = i\n",
    "        break\n",
    "print \"Tracy-Widom test yields %d axes of pop structure\" % tw_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Hmmm, that seems like a lot.  Let's fix it with some outlier exclusion.  \n",
    "\n",
    "Can you describe what might be happening here?  If not, maybe it's in the paper..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = pd.DataFrame(x)\n",
    "for col in y.columns[0:10]:\n",
    "    s_cutoff = np.std(y[col])*6\n",
    "    u = np.mean(y[col])\n",
    "    cutoff = sorted([u+s_cutoff, u-s_cutoff], reverse=True)\n",
    "    outliers = y[col][(y[col] > cutoff[0]) | (y[col] < cutoff[1])]\n",
    "    print col\n",
    "    print outliers\n",
    "    y = y.drop(outliers.index)\n",
    "y.ix[0:5,0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "genotypes = hdf.get(\"genotypes\")\n",
    "gt_drop = genotypes.ix[y.index,:]\n",
    "gt_drop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "af = hdf.get(\"af\")\n",
    "z12_drop = gt_drop.apply(convert_to_z12)\n",
    "z12_drop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca_drop_std = z12_drop.apply(center_and_standardize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdf.put('pca_drop_std', pca_drop_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prcomp_res_drop = prcomp(pca_drop_std, scale=False, center=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_drop = com.convert_robj(prcomp_res_drop.rx2(\"x\"))\n",
    "x_drop.index = pca_drop_std.index\n",
    "x_drop.ix[0:5,0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(x_drop.PC1, x_drop.PC2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print summary(prcomp_res_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tw = TWcalc(com.convert_to_r_matrix(pca_drop_std), 25)\n",
    "tw_p = com.convert_robj(tw.rx2(2))\n",
    "tw_e = com.convert_robj(tw.rx2(1))\n",
    "tw_num = 0\n",
    "for i, p in enumerate(tw_p):\n",
    "    print i, p\n",
    "    if p > 0.05:\n",
    "        tw_num = i\n",
    "        break\n",
    "print \"Tracy-Widom test yields %d axes of pop structure\" % tw_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####OK, that was a lot of work you just did.  To recap:\n",
    "\n",
    "1. We found the outliers based on some criteria (Did you figure it out/look it up?)\n",
    "1. We removed those outliers from the raw base/base data\n",
    "1. We normalized the data again, performed PCA, and tested with T-W.  \n",
    "\n",
    "How many axes of pop structure are left now?  Does that seem reasonable?\n",
    "\n",
    "####The next section deals with hierfstat.  Sam already talked a little bit about this, but...\n",
    "\n",
    "1. We have to transform our data again to their format\n",
    "1. We're going to us the 012 data that does not contain outliers.\n",
    "1. We're going to stash a copy of the input data for later use with `Bayenv`.  I do this because I care about you.\n",
    "1. Because hierfstat knows about populations, we need to get that data in there, too.  Run the next few cells until that's done.  You'll know it's done when you write the `hierf.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hierf_trans = {0:11, 1:12, 2:22, -1:'NA'}\n",
    "def apply_hierf_trans(series):\n",
    "    return [hierf_trans[x] if x in hierf_trans else x for x in series]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hierf_df = z12_drop.apply(apply_hierf_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hierf_df.insert(0, \"countyid\", None)\n",
    "hierf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_loc = hdf.get(\"data_loc\")\n",
    "loc_hierf = data_loc.join(hierf_df, how=\"inner\")\n",
    "loc_hierf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdf.put(\"loc_hierf\", loc_hierf)\n",
    "hdf.put(\"bayenv_df\", loc_hierf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loc_hierf['county_state'] = loc_hierf.apply(lambda row: \"%s_%s\" % (row.county, row.state), axis=1)\n",
    "usable_counties = set()\n",
    "county_counts = loc_hierf.county_state.value_counts()\n",
    "county_counts = county_counts.sort(inplace=False, ascending=False)\n",
    "for c in county_counts.index:\n",
    "    print c, county_counts[c]\n",
    "for c in county_counts.index:\n",
    "    if county_counts[c] >=5:\n",
    "        usable_counties.add(c)\n",
    "usable_counties = sorted(list(usable_counties))\n",
    "\n",
    "county_id = {}\n",
    "for i, county in enumerate(usable_counties):\n",
    "    county_id[county] = i+1\n",
    "county_id\n",
    "\n",
    "loc_hierf['usable'] = loc_hierf.apply(lambda row: row.county_state in county_id, axis=1)\n",
    "\n",
    "drop = loc_hierf[loc_hierf.usable==False]\n",
    "\n",
    "loc_hierf = loc_hierf.drop(drop.index)\n",
    "\n",
    "loc_hierf['countyid'] = loc_hierf.apply(lambda row: county_id[row.county_state], axis=1)\n",
    "\n",
    "loc_hierf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dill.dump(county_id, open(\"county_id.dill\",\"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loc_hierf.ix[:,4:-2].to_csv(\"hierf.txt\", sep=\"\\t\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've already run this for you, but just in case you wanted to do something like it in the future, the code is below.  Expect it to take a few hours, or days/weeks for huge datasets.\n",
    "\n",
    "\n",
    "```\n",
    "library(hierfstat)\n",
    "data = read.table(\"hierf.txt\", header=T, sep=\"\\t\")\n",
    "data = data[order(data$countyid),]\n",
    "levels = data.frame(data$countyid)\n",
    "loci = data[,2:ncol(data)]\n",
    "bs = basic.stats(data)\n",
    "saveRDS(bs, \"basic_stats.rds\")\n",
    "res = varcomp.glob(levels=levels, loci=loci, diploid=T)\n",
    "saveRDS(res, \"hierf.rds\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Let's read that hierfstat into R, so we can get it into Python.  \n",
    "\n",
    "No seriously, it's better.  Trust me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "bs = readRDS(\"basic_stats.rds\")\n",
    "res = readRDS(\"hierf.rds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = com.convert_robj(ro.r('res'))\n",
    "bs = com.convert_robj(ro.r('bs'))\n",
    "Fis = bs['Fis']\n",
    "Hs = bs['Hs']\n",
    "pop_freq_temp = bs['pop.freq']\n",
    "pop_freq = {}\n",
    "perloc = bs['perloc']\n",
    "n_ind_samp = bs['n.ind.samp']\n",
    "Ho = bs['Ho']\n",
    "overall = bs['overall']\n",
    "\n",
    "for df in [Fis, Hs, perloc, n_ind_samp, Ho]:\n",
    "    df.index = [x[1:].replace(\".\",\"-\") for x in df.index]\n",
    "\n",
    "for locus, data in pop_freq_temp.items():\n",
    "    if len(data) == 2:\n",
    "        data.index = ['p','q']\n",
    "    else:\n",
    "        data.index = ['p']\n",
    "    pop_freq[locus[1:].replace(\".\", \"-\")] = data\n",
    "\n",
    "Ho = Ho.T\n",
    "perloc = perloc.T\n",
    "n_ind_samp = n_ind_samp.T\n",
    "Hs = Hs.T\n",
    "Fis = Fis.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####You know have matrices for Ho, perloc, n_ind_samp, Hs, and Fis from hierfstat available as `DataFrame`s.  \n",
    "\n",
    "Add some cells and explore them with head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loc_df = res['loc']\n",
    "F_df = res['F']\n",
    "overall_df = res['overall']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####What data is contained in the F matrix (`F_df`)?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "F_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####We can compute $F_{ST}$ for each site, as well, from the variance components in `loc_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_fst(series):\n",
    "    Va = series[0]\n",
    "    Vt = sum(series)\n",
    "    return Va/Vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loci_fst = loc_df.apply(compute_fst, axis=1).dropna()\n",
    "loci_fst.index = [x[1:].replace(\".\", \"-\") for x in loci_fst.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(loci_fst, bins=50)\n",
    "plt.xlim(-.03, .5)\n",
    "plt.title(\"Fst for %d loci $(\\mu=%.3f \\pm %.4f) [%.3f, %.3f]$\" % (len(loci_fst),\n",
    "                                                                  np.mean(loci_fst),\n",
    "                                                                  np.std(loci_fst),\n",
    "                                                                  np.min(loci_fst),\n",
    "                                                                  np.max(loci_fst)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loci_fst[loci_fst>0.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Let's now join up our phenotype data with our hierfstat data.\n",
    "\n",
    "You'll often find that you'll have to do this, especially when working in collaboration across groups.  It's not ideal, as you'll see, because not everything matches up.  Oh well, moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_phenotype(row):\n",
    "    return np.max(pheno[(pheno.Longitude==row.long) & (pheno.Latitude==row.lat)])\n",
    "\n",
    "pheno = hdf.get(\"pheno\")\n",
    "trait = loc_hierf.apply(get_phenotype, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trait.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Inner, outer (left, right) joins are possible in Python, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trait_loc_hierf = trait.join(loc_hierf, how=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Let's drop populations that do not have data for your favorite trait.\n",
    "\n",
    "Remember saving that file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trait_name = dill.load(open(\"trait_name.dill\"))\n",
    "trait_complete = trait_loc_hierf.drop(trait_loc_hierf[np.isnan(trait_loc_hierf[trait_name])].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Let's set up our PCA matrix to only have the axes that relate to our population structure, and save the rest of the data out for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca_cov = x_drop.ix[:,0:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdf.put(\"trait_complete\", trait_complete)\n",
    "hdf.put(\"x_drop\", x_drop)\n",
    "hdf.put(\"pca_cov\", pca_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trait_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key, df in {\"Fis\":Fis, \n",
    "                \"Hs\":Hs, \n",
    "                \"perloc\": perloc, \n",
    "                \"n_ind_samp\":n_ind_samp, \n",
    "                \"Ho\":Ho}.items():\n",
    "    hdf.put(key, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdf.put(\"trait_complete\", trait_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}