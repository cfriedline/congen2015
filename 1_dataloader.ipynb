{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Exploring Patterns of Population Structure and Environmental Associations to Aridity Across the Range of Loblolly Pine\n",
    "\n",
    "##Introduction\n",
    "\n",
    "In this set of analyses, we will be making use of data from the Eckert et al. 2010 paper to explore patterns of phenotypic and environmental associations among populations of loblolly pine.\n",
    "\n",
    "\n",
    "###Abstract\n",
    "\n",
    "Natural populations of forest trees exhibit striking phenotypic adaptations to diverse environmental\n",
    "gradients, thereby making them appealing subjects for the study of genes underlying ecologically relevant phenotypes. Here, we use a genome-wide data set of single nucleotide polymorphisms genotyped across 3059 functional genes to study patterns of population structure and identify loci associated with aridity across the natural range of loblolly pine (Pinus taeda L.). Overall patterns of population structure, as inferred using principal components and Bayesian cluster analyses, were consistent with three genetic clusters likely resulting from expansions out of Pleistocene refugia located in Mexico and Florida. A novel application of association analysis, which removes the confounding effects of shared ancestry on correlations between genetic and environmental variation, identified five loci correlated with aridity. These loci were primarily involved with abiotic stress response to temperature and drought. A unique set of 24 loci was identified as FST outliers on the basis of the genetic clusters identified previously and after accounting for expansions out of Pleistocene refugia. These loci were involved with a diversity of physiological processes. Identification of nonoverlapping sets of loci highlights the fundamental differences implicit in the use of either method and suggests a pluralistic, yet complementary, approach to the identification of genes underlying ecologically relevant phenotypes.\n",
    "\n",
    "\n",
    "##Overview of tasks\n",
    "\n",
    "In general, what you will be doing is working your way from loading and saving data related to this study, to corrections for population structure, to looking for associations between genotypes and phenotypes, genotypes and the environment (`Bayenv2`), and genotypes+phenotypes+environment (`SQUAT`)\n",
    "\n",
    "## This notebook\n",
    "\n",
    "The goal of this notebook is to get set up and explore some basic patterns of the data.  At many of the steps, there is also a sanity check so that a quick view of the data (via the `head()` function of a `Pandas` dataframe).\n",
    "\n",
    "As you work your way through these notebooks, please keep in mind that there are several things that are going on:\n",
    "\n",
    "1. You are getting exposed to an amazing technology through the use of the IPython Notebook.\n",
    "1. You are learning a little bit of Python.\n",
    "1. You are getting familiar with the ways in which I actually do my analysis.\n",
    "1. You are taking with you about 75% of the processes needed to publish GWAS results on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Let's begin\n",
    "\n",
    "At the top of every notebook, there is a cell of code which sets up some basic functionality.  At this point, it's sufficient to just run the cell and move on.  If you want to know what things do, please ask.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from IPython.display import Image\n",
    "import pandas as pd\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import rpy2\n",
    "from rpy2 import robjects as ro\n",
    "import pandas.rpy.common as com\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import operator\n",
    "import scipy as sp\n",
    "import traceback\n",
    "from sklearn import preprocessing\n",
    "from IPython.parallel import Client\n",
    "from subprocess import Popen, PIPE\n",
    "import shutil\n",
    "from IPython.display import FileLink, FileLinks, Image\n",
    "import psutil\n",
    "import multiprocessing\n",
    "from hdfstorehelper import HDFStoreHelper\n",
    "import warnings\n",
    "import pandas\n",
    "import dill\n",
    "warnings.simplefilter(\"ignore\", pandas.io.pytables.PerformanceWarning)\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext rpy2.ipython\n",
    "pd.set_option('display.width', 80)\n",
    "pd.set_option('max.columns', 30)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Execute the cell below to lead in the phenotype data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pheno = pd.read_excel(\"/gdc_home5/groups/congenomics/day5/landscape_genetics_data/Pinus_taeda_metabolite_data.xlsx\", \n",
    "                      sheetname=\"metabolite_phenotype_data\",\n",
    "                      header=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Let's look at all the traits in the file that contain the phrase, 'ose' (e.g., glucose, sucrose, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "traits = [x for x in pheno.columns if \"ose\" in str(x)]\n",
    "print traits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####In the cell above, you should see the name of your new favorite metabolite.  Pick your favorite one, and set it's value in the cell.\n",
    "\n",
    "For example, to set the `trait_name` variable to `maltose`, you could do one of the following:\n",
    "\n",
    "* `trait_name = \"maltose\"`\n",
    "* `trait_name = traits[4]`\n",
    "\n",
    "After you set `trait_name` to your favorite one, execute the cell.  The notebook will also dump a file that we can use later across notebooks, which remember, are separate Python processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trait_name = \"fructose\"\n",
    "dill.dump(trait_name, open(\"trait_name.dill\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####We need to tweak the phenotype data a bit for later analysis, so run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pheno = pheno[['Longitude', 'Latitude','Clone_id',trait_name]]\n",
    "pheno.index = pheno.Clone_id\n",
    "pheno = pheno.drop('Clone_id', axis=1)\n",
    "pheno = pheno.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Have a peek a the the phenotype data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pheno.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Load the rest of the data.  This will probably take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_ai = pd.read_excel(\"/gdc_home5/groups/congenomics/day5/landscape_genetics_data/Genetics_2010/Eckert_Genetics_2010_data.xlsx\")\n",
    "data_gt = pd.read_excel(\"/gdc_home5/groups/congenomics/day5/landscape_genetics_data/Genetics_2010/Eckert_Genetics_2010_data.xlsx\", \n",
    "                        sheetname=\"genotyping_data\")\n",
    "data_loc = pd.read_excel(\"/gdc_home5/groups/congenomics/day5/landscape_genetics_data/Genetics_2010/Eckert_Genetics_2010_data.xlsx\",\n",
    "                         sheetname=\"county_locality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_county_state_key(df, c, s):\n",
    "    d = df.copy()\n",
    "    d['county_state'] = d.apply(lambda row: \"%s_%s\" % (row[c], row[s]), axis=1)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###The next three cells massage the data a bit more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_ai = add_county_state_key(data_ai, \"County\", \"State\")\n",
    "data_ai.ix[:,0:2] = data_ai.ix[:,0:2].astype(str)\n",
    "data_ai.ix[:,2:-1] = data_ai.ix[:,2:-1].astype(float)\n",
    "data_ai.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_loc = add_county_state_key(data_loc, \"county\", \"state\")\n",
    "data_loc.ix[:,0:2] = data_loc.ix[:,0:2].astype(str)\n",
    "data_loc.ix[:,2:-1] = data_loc.ix[:,2:-1].astype(float)\n",
    "data_loc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_gt = data_gt.astype(str)\n",
    "data_gt = add_county_state_key(data_gt, \"county\", \"state\")\n",
    "data_gt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####We'll be using HDF5 to store our data across notebooks, in addition to the `dill` serialization above.\n",
    "\n",
    "This is a widely used technology for large and hierarchical data sets, though we won't really be using it that way.  If you want, you can read more about this technology from [HDF5 themselves](http://www.hdfgroup.org/HDF5/) or [PyTables](https://pytables.github.io/usersguide/tutorials.html).  Run through these `.put()` cells to save the data out to disk for later use.\n",
    "\n",
    "You may also be wondering about the `HDFStoreHelper` module.  I wrote that to save myself some time and safety for common HDF5 things.  Feel free to make it your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdf = HDFStoreHelper(\"data.hd5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdf.put(\"pheno\", pheno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdf.put(\"data_ai\", data_ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdf.put(\"data_loc\", data_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdf.put(\"data_gt\", data_gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Sometimes it's nice to have genotype data all by itself.  Let's filter our data down to just that by using the naming pattern in the columns of `data_gt`.\n",
    "\n",
    "Run these cells to create the `DataFrame`, view a bit of it, and then save it to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "genotypes = data_gt.ix[:,[x for x in data_gt.columns if '-' in x]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "genotypes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdf.put(\"genotypes\", genotypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Let's load up some functions to some work later.  I feel like even though they're not commented, it's still pretty readable what it does.  \n",
    "\n",
    "Feel free to browse the code and ask questions.  Not all programmers are the same, so the way my brain works might not make sense to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_homozygous(gt):\n",
    "    if len(set([x.strip() for x in gt.split(\"/\")])) == 1:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def get_allele_counts(counts):\n",
    "    a = {}\n",
    "    het = 0\n",
    "    for gt in counts.index:\n",
    "        for allele in [x.strip() for x in gt.split(\"/\")]:\n",
    "            if not allele in a:\n",
    "                a[allele] = 0\n",
    "            a[allele] += counts[gt]\n",
    "        if not is_homozygous(gt):\n",
    "            het += counts[gt]\n",
    "    return sorted(a.items(), key=lambda x: x[1], reverse=True), het\n",
    "\n",
    "def get_correction(n):\n",
    "    #for finite sample size\n",
    "    return (2*n)/(2*n-1)\n",
    "\n",
    "def get_allele_freqs(locus):\n",
    "    locus = locus[locus != '?/?']\n",
    "    locus = locus[locus != 'NA']\n",
    "    c = locus.value_counts()\n",
    "    c = c.sort(inplace=False, ascending=False)\n",
    "    allele_counts = get_allele_counts(c)\n",
    "    total_alleles = 2.0*sum(c)\n",
    "    num_individuals = sum(c)\n",
    "    A = \"\"\n",
    "    a = \"\"\n",
    "    P = 0\n",
    "    Q = 0\n",
    "    if len(allele_counts[0]) == 2:\n",
    "        A = allele_counts[0][0][0]\n",
    "        a = allele_counts[0][1][0]\n",
    "        P = allele_counts[0][0][1]\n",
    "        Q = allele_counts[0][1][1]\n",
    "    else:\n",
    "        A = allele_counts[0][0][0]\n",
    "        P = P = allele_counts[0][0][1]\n",
    "    PQ = allele_counts[-1]\n",
    "    p = P/total_alleles\n",
    "    q = Q/total_alleles\n",
    "    assert p + q == 1.0\n",
    "    He = 2 * p * q * get_correction(num_individuals)\n",
    "    Ho = PQ*1.0/num_individuals\n",
    "    Fis = 1 - (Ho/He)\n",
    "    #print p, q, He, Ho, Fis\n",
    "    ret = pd.Series({\"p\":p, \n",
    "                      \"q\":q,\n",
    "                      \"P\":P,\n",
    "                      \"Q\":Q,\n",
    "                      \"He\":He,\n",
    "                      \"Ho\":Ho, \n",
    "                      \"Fis\":Fis,\n",
    "                    \"PQ\": PQ,\n",
    "                    \"total_alleles\":total_alleles,\n",
    "                    \"num_indiv\":num_individuals,\n",
    "                    \"A\":A,\n",
    "                    \"a\":a})\n",
    "    return ret\n",
    "\n",
    "def plot_hist(df, index):\n",
    "    d = df.ix[index,:]\n",
    "    plt.hist(d, bins=20)\n",
    "    plt.title(\"%s %.2f $\\pm$ %.3f [%.2f, %.2f]\" % (index, \n",
    "                                                   np.mean(d), \n",
    "                                                   np.std(d),\n",
    "                                                  np.min(d),\n",
    "                                                  np.max(d)))\n",
    "    \n",
    "def convert_to_z12(locus):\n",
    "    freq = af[locus.name]\n",
    "    trans = {\"%s/%s\" % (freq[\"A\"],freq[\"A\"]): 0,\n",
    "            \"%s/%s\" % (freq[\"a\"],freq[\"a\"]): 2,\n",
    "            \"%s/%s\" % (freq[\"A\"],freq[\"a\"]): 1,\n",
    "            \"%s/%s\" % (freq[\"a\"],freq[\"A\"]): 1,\n",
    "            \"?/?\":-1}\n",
    "    return locus.apply(lambda x: trans[x])\n",
    "\n",
    "def center_and_standardize_value(val, u, var):\n",
    "    if val == -1:\n",
    "        return 0.0\n",
    "    return (val-u)/np.sqrt(var)\n",
    "\n",
    "def center_and_standardize(snp):\n",
    "    maf = af.ix[\"q\",snp.name]\n",
    "    u = np.mean([x for x in snp if x != -1])\n",
    "    var = np.sqrt(maf*(1-maf))\n",
    "    return snp.apply(center_and_standardize_value, args=(u, var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####This next set of cells does a few things:\n",
    "\n",
    "1. Creates a `DataFrame` to hold some allele frequency data, saves it, and plots Fis from it.\n",
    "1. Coverts the base/base genotypes into a 0/1/2 matrix, like the one you might get out of `vcftools`.  Note that 2's are the minor allele.  -1 represents missing data.\n",
    "1. Takes the 012 file and applies normalization to it (subtracting the value form the mean genotype and dividing by the variance of the minor allele frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "af = genotypes.apply(get_allele_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "af.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdf.put(\"af\", af)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_hist(af, \"Fis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z12 = genotypes.apply(convert_to_z12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdf.put(\"z12\", z12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca_std = z12.apply(center_and_standardize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdf.put(\"pca_std\", pca_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z12.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####This is the standardization from Patterson et al. 2006, I talked about in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca_std.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}