{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Exploring Patterns of Population Structure and Environmental Associations to Aridity Across the Range of Loblolly Pine\n",
    "\n",
    "##Introduction\n",
    "\n",
    "In this set of analyses, we will be making use of data from the Eckert et al. 2010 paper to explore patterns of phenotypic and environmental associations among populations of loblolly pine.\n",
    "\n",
    "\n",
    "###Abstract\n",
    "\n",
    "Natural populations of forest trees exhibit striking phenotypic adaptations to diverse environmental\n",
    "gradients, thereby making them appealing subjects for the study of genes underlying ecologically relevant phenotypes. Here, we use a genome-wide data set of single nucleotide polymorphisms genotyped across 3059 functional genes to study patterns of population structure and identify loci associated with aridity across the natural range of loblolly pine (Pinus taeda L.). Overall patterns of population structure, as inferred using principal components and Bayesian cluster analyses, were consistent with three genetic clusters likely resulting from expansions out of Pleistocene refugia located in Mexico and Florida. A novel application of association analysis, which removes the confounding effects of shared ancestry on correlations between genetic and environmental variation, identified five loci correlated with aridity. These loci were primarily involved with abiotic stress response to temperature and drought. A unique set of 24 loci was identified as FST outliers on the basis of the genetic clusters identified previously and after accounting for expansions out of Pleistocene refugia. These loci were involved with a diversity of physiological processes. Identification of nonoverlapping sets of loci highlights the fundamental differences implicit in the use of either method and suggests a pluralistic, yet complementary, approach to the identification of genes underlying ecologically relevant phenotypes.\n",
    "\n",
    "\n",
    "##Overview of tasks\n",
    "\n",
    "In general, what you will be doing is working your way from loading and saving data related to this study, to corrections for population structure, to looking for associations between genotypes and phenotypes, genotypes and the environment (`Bayenv2`), and genotypes+phenotypes+environment (`SQUAT`)\n",
    "\n",
    "## This notebook\n",
    "\n",
    "This notebook gets you working with Bayenv2 data.  I've done a lot of the upfront work for you, so just worry about interpretation rather than running things.  Code to run is included, of course.\n",
    "\n",
    "As with the previous notebook, execute the cell with the imports and continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from IPython.display import Image\n",
    "import pandas as pd\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import rpy2\n",
    "from rpy2 import robjects as ro\n",
    "import pandas.rpy.common as com\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import operator\n",
    "import scipy as sp\n",
    "import traceback\n",
    "from sklearn import preprocessing\n",
    "from IPython.parallel import Client\n",
    "from subprocess import Popen, PIPE\n",
    "import shutil\n",
    "from IPython.display import FileLink, FileLinks, Image\n",
    "import psutil\n",
    "import multiprocessing\n",
    "from hdfstorehelper import HDFStoreHelper\n",
    "import warnings\n",
    "import pandas\n",
    "import dill\n",
    "import statsmodels as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy.stats.stats import pearsonr\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib as mpl\n",
    "warnings.simplefilter(\"ignore\", pandas.io.pytables.PerformanceWarning)\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext rpy2.ipython\n",
    "pd.set_option('display.width', 80)\n",
    "pd.set_option('max.columns', 30)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = ro.r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdf = HDFStoreHelper(\"data.hd5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trait_name = str(dill.load(open(\"trait_name.dill\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trait_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Let's load that Bayenv data from before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bayenv_df = hdf.get(\"bayenv_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bayenv_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####We need to add county and stuff back in, as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "county_id = dill.load(open(\"county_id.dill\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_county_id(row):\n",
    "    key = \"%s_%s\" % (row.county,row.state)\n",
    "    if key in county_id:\n",
    "        return county_id[key]\n",
    "    return np.nan\n",
    "bayenv_df['countyid'] = bayenv_df.apply(add_county_id, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bayenv_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bayenv_df = bayenv_df[bayenv_df.countyid > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bayenv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert_to_snpassoc(col):\n",
    "    if \"-\" in col.name:\n",
    "        freqs = af[col.name]\n",
    "        trans = {11: \"%s/%s\" % (freqs[\"A\"], freqs[\"A\"]),\n",
    "                12: \"%s/%s\" % (freqs[\"A\"], freqs[\"a\"]),\n",
    "                22: \"%s/%s\" % (freqs[\"a\"], freqs[\"a\"]),\n",
    "                \"NA\":\"NA\"}\n",
    "        return col.apply(lambda x: trans[x])\n",
    "    return col\n",
    "\n",
    "def is_homozygous(gt):\n",
    "    if len(set([x.strip() for x in gt.split(\"/\")])) == 1:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def get_allele_counts(counts):\n",
    "    a = {}\n",
    "    het = 0\n",
    "    for gt in counts.index:\n",
    "        alleles = None\n",
    "        if \"/\" in gt:\n",
    "            alleles = [x.strip() for x in gt.split(\"/\")]\n",
    "        else:\n",
    "            alleles = gt\n",
    "        for allele in alleles:\n",
    "            if not allele in a:\n",
    "                a[allele] = 0\n",
    "            a[allele] += counts[gt]\n",
    "        if not is_homozygous(gt):\n",
    "            het += counts[gt]\n",
    "    return sorted(a.items(), key=lambda x: x[1], reverse=True), het\n",
    "\n",
    "\n",
    "def get_correction(n):\n",
    "    #for finite sample size\n",
    "    return (2*n)/(2*n-1)\n",
    "\n",
    "def get_allele_freqs(locus):\n",
    "    locus = locus[locus != '?/?']\n",
    "    locus = locus[locus != 'NA']\n",
    "    c = locus.value_counts()\n",
    "    c = c.sort(inplace=False, ascending=False)\n",
    "    allele_counts = get_allele_counts(c)\n",
    "    total_alleles = 2.0*sum(c)\n",
    "    num_individuals = sum(c)\n",
    "    A = \"\"\n",
    "    a = \"\"\n",
    "    P = 0\n",
    "    Q = 0\n",
    "    if len(allele_counts[0]) == 2:\n",
    "        A = allele_counts[0][0][0]\n",
    "        a = allele_counts[0][1][0]\n",
    "        P = allele_counts[0][0][1]\n",
    "        Q = allele_counts[0][1][1]\n",
    "    else:\n",
    "        A = allele_counts[0][0][0]\n",
    "        P = P = allele_counts[0][0][1]\n",
    "    PQ = allele_counts[-1]\n",
    "    p = P/total_alleles\n",
    "    q = Q/total_alleles\n",
    "    assert p + q == 1.0\n",
    "    He = 2 * p * q * get_correction(num_individuals)\n",
    "    Ho = PQ*1.0/num_individuals\n",
    "    Fis = 1 - (Ho/He)\n",
    "    #print p, q, He, Ho, Fis\n",
    "    ret = pd.Series({\"p\":p, \n",
    "                      \"q\":q,\n",
    "                      \"P\":P,\n",
    "                      \"Q\":Q,\n",
    "                      \"He\":He,\n",
    "                      \"Ho\":Ho, \n",
    "                      \"Fis\":Fis,\n",
    "                    \"PQ\": PQ,\n",
    "                    \"total_alleles\":total_alleles,\n",
    "                    \"num_indiv\":num_individuals,\n",
    "                    \"A\":A,\n",
    "                    \"a\":a})\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "af = hdf.get(\"af\")\n",
    "bayenv_df = bayenv_df.apply(convert_to_snpassoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####We need to get allele frequencies by population\n",
    "\n",
    "This also takes a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pop_allele_freqs = {}\n",
    "for pop,data in bayenv_df.groupby(\"countyid\"):\n",
    "    print \"getting allele freqs for pop % d\" % pop\n",
    "    pop_allele_freqs[pop] = data.ix[:,6:].apply(get_allele_freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####The allele freqs data from above also returns counts of each allele, which is really what we need for Bayenv.  \n",
    "\n",
    "Remember that data we called, `af` before?  This is the same, but for each population.  I call the counts of major and minor allele, P and Q, and their frequencies p and q, respectively.  Probably a bad habit, I admit.\n",
    "\n",
    "Let's get those counts for the SNPs and put it into a file that Bayenv can understand.  The Bayenv file format also sucks, and it's worth saying it to all of your friends.  Who ends lines with a tab?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bayenv_snp(snp_name, popids):\n",
    "    P = []\n",
    "    Q = []\n",
    "    for popid in popids:\n",
    "        P.append(pop_allele_freqs[popid].ix[\"P\",name])\n",
    "        Q.append(pop_allele_freqs[popid].ix[\"Q\",name])\n",
    "    return P, Q\n",
    "\n",
    "def write_bayenv_snp(fh_snp, fh_names, name, P, Q):\n",
    "    if sum(Q) > 0: #exclude monomorphic loci\n",
    "        if fh_names:\n",
    "            fh_names.write(\"%s\\n\" % name)\n",
    "        P = [str(x) for x in P]\n",
    "        Q = [str(x) for x in Q]\n",
    "        fh_snp.write(\"%s\\t\\n\" % \"\\t\".join(Q))\n",
    "        fh_snp.write(\"%s\\t\\n\" % \"\\t\".join(P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bayenv_dir = \"bayenv\"\n",
    "snp_names = [x for x in bayenv_df.columns if \"-\" in x]\n",
    "popids = sorted(bayenv_df.countyid.unique())\n",
    "\n",
    "if not os.path.exists(bayenv_dir):\n",
    "    os.mkdir(bayenv_dir)\n",
    "\n",
    "with open(\"bayenv.txt\", \"w\") as o:\n",
    "    with open(\"bayenv_names.txt\", \"w\") as n:\n",
    "        for name in snp_names:\n",
    "            P,Q = get_bayenv_snp(name, popids)\n",
    "            write_bayenv_snp(o, n, name, P, Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Let's look at the first two (wrapped) lines of the file.  \n",
    "\n",
    "Look familiar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head -n2 bayenv.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Copy that file into your bayenv directory for use later, as below.  \n",
    "\n",
    "Have you noticed that IPython can also call shell commands on the server?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cp bayenv.txt bayenv/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Run Bayenv to create variance-covariance matrix\n",
    "\n",
    "```bash\n",
    "    cd bayenv && /gdc_home4/cfried/src/bayenv2/bayenv2 -i bayenv.txt -p 30 -k 100000 -r 63479 > matrix.out\n",
    "```\n",
    "\n",
    "* -p number of populations (`len(popids)`)\n",
    "* -k mcmc generations\n",
    "* -r random seed\n",
    "\n",
    "This has already been done for you since it takes several hours for 100,000 generations.  This same data takes about 15 hours for 1,000,000 generations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####As with all things Bayesian, it's good to look at the data to make sure that the MCMC chain is mixing properly.\n",
    "\n",
    "This is just one way, using pearson correlation, and checking out the correlation between the last matrix and all previous ones.  You can imagine any number of scenarios, I'm sure, but it seems to overall converge very quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vcovs = []\n",
    "current = None\n",
    "for line in open(\"bayenv/matrix.out\"):\n",
    "    if \"VAR-COVAR\" in line:\n",
    "        current = []\n",
    "        vcovs.append(current)\n",
    "    if isinstance(current, list):\n",
    "        current.append(line.strip().split(\"\\t\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vcov_dfs = []\n",
    "for i, elem in enumerate(vcovs):\n",
    "    vcov_dfs.append(pd.DataFrame(vcovs[i][1:]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matrix_correlation = np.zeros((len(vcovs), len(vcovs)))\n",
    "for i in xrange(len(vcovs)):\n",
    "    if i == len(vcovs)-1:\n",
    "        for j in xrange(i+1):\n",
    "            idf = vcov_dfs[i]\n",
    "            jdf = vcov_dfs[j]\n",
    "            idf = idf.ix[:,:len(idf)-1]\n",
    "            jdf = jdf.ix[:,:len(jdf)-1]\n",
    "            idf = [float(x) for x in idf.values.flatten()]\n",
    "            jdf = [float(x) for x in jdf.values.flatten()]\n",
    "            assert len(idf) == len(jdf)\n",
    "            matrix_correlation[i, j] = sp.stats.pearsonr(idf, jdf)[0]\n",
    "            matrix_correlation[j, i] = matrix_correlation[i, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ys = []\n",
    "for i in xrange(len(matrix_correlation)):\n",
    "    for j in xrange(i):\n",
    "        if i == len(vcovs)-1:\n",
    "            ys.append(matrix_correlation[i,j])\n",
    "plt.plot(range(len(ys)),ys)\n",
    "plt.title(\"Pearson correlations among %d adjacent VCOV matrices\" % len(matrix_correlation))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Does this tell you anything about the process?  Are you comfortable choosing the last matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####One way to view the covariance structure of a matrix (a moderately sized one) is a heatmap. \n",
    "\n",
    "In Python, here's one way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vcov = pd.read_csv(\"bayenv/matrix_last.out\", sep=\"\\t\", header=None)\n",
    "vcov = vcov.ix[:,:len(vcov.columns)-2]\n",
    "sns.heatmap(vcov)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Bayenv mcmc\n",
    "\n",
    "The code below will walk you through getting this all set up, but it will not run it. Bayenv is best run with GNU `parallel` I've found, and also pinning to single CPUs because of how their software is programmed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_ai = hdf.get(\"data_ai\")\n",
    "data_ai['county_state'] = data_ai.apply(lambda row: \"%s_%s\" % (row.County, row.State), axis=1)\n",
    "bayenv_df_ai = bayenv_df.merge(data_ai, on='county_state')\n",
    "bayenv_df_ai.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bayenv_df_ai.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_bayenv_env(data):\n",
    "    E = pd.Series()\n",
    "    for col in data.columns[:-1]:\n",
    "        E[col] = data[col].values[0]\n",
    "    return E\n",
    "\n",
    "ai_cols = [x for x in bayenv_df_ai if 'AI_' in x]\n",
    "ai_cols.append('countyid')\n",
    "bayenv_df_ai_groups = bayenv_df_ai.ix[:,ai_cols].groupby(\"countyid\")\n",
    "env_ai = []\n",
    "for popid in popids:\n",
    "    env_ai.append(get_bayenv_env(bayenv_df_ai_groups.get_group(popid))) \n",
    "env_ai_df = pd.DataFrame(env_ai).T\n",
    "env_ai_df = env_ai_df.apply(preprocessing.scale, axis=1)\n",
    "env_ai_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_last_column(row):\n",
    "    row[len(row)] = \"\"\n",
    "    return row \n",
    "\n",
    "env_ai_df.apply(add_last_column, axis=1).to_csv(\"envmatrix.txt\", \n",
    "                                               header=None,\n",
    "                                                index=True,\n",
    "                                               sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cp bayenv/matrix_last.out ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def setup_bayenv_cmd(snpfile, name):\n",
    "    work_dir = os.path.join(os.path.abspath(\".\"), \"bayenv\")\n",
    "    bayenv = \"/gdc_home4/cfried/src/bayenv2/bayenv2\"\n",
    "    bayenv_matrix = \"matrix_last.out\"\n",
    "    bayenv_seed = -47372\n",
    "    bayenv_pops = 12\n",
    "    bayenv_runs = 100000\n",
    "    bayenv_environs = 4\n",
    "    bayenv_envmatrix = \"envmatrix.txt\"\n",
    "    bayenv_cmd = \"cd %s/%s && %s -i %s -m %s -e %s -p %d -k %d -n %d -t -c -f -X -o %s\" % (work_dir, \n",
    "                                                                                        name,\n",
    "                                                                                        bayenv,\n",
    "                                                                         snpfile,\n",
    "                                                                         bayenv_matrix,\n",
    "                                                                         bayenv_envmatrix,\n",
    "                                                                         bayenv_pops,\n",
    "                                                                     bayenv_runs,\n",
    "                                                                     bayenv_environs,\n",
    "                                                                             snpfile)\n",
    "    shutil.copy(bayenv_matrix, os.path.join(work_dir, name))\n",
    "    shutil.copy(bayenv_envmatrix, os.path.join(work_dir, name))\n",
    "    return bayenv_cmd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cmds = []\n",
    "if not os.path.exists(bayenv_dir):\n",
    "    os.mkdir(bayenv_dir)\n",
    "\n",
    "for name in snp_names:\n",
    "    P,Q = get_bayenv_snp(name,popids)\n",
    "    if sum(Q) > 0:\n",
    "        file_dir = os.path.join(bayenv_dir, name)        \n",
    "        if not os.path.exists(file_dir):\n",
    "            os.mkdir(file_dir)\n",
    "        o = open(os.path.join(file_dir, \"%s.txt\" % name), \"w\")\n",
    "        write_bayenv_snp(o, None, name, P, Q)\n",
    "        o.close()\n",
    "        cmd = setup_bayenv_cmd(os.path.basename(o.name), name)\n",
    "        cmds.append(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"bayenv_jobs.txt\", \"w\") as o:\n",
    "    cpu = 0\n",
    "    max_cpus = 20\n",
    "    for cmd in cmds:\n",
    "        c = cmd.split()\n",
    "        c[2] = \"&& taskset -c %d\" % cpu\n",
    "        o.write(\"%s\\n\" % \" \".join(c))\n",
    "        cpu += 1\n",
    "        if cpu == max_cpus:\n",
    "            cpu = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run all the commands\n",
    "\n",
    "But don't, I already did it for you.\n",
    "\n",
    "```bash\n",
    "cat bayenv_jobs.txt | parallel -j 20 --eta --\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####One of the commands looks like this, if you're curious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cmds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####We can iterate the bayenv directory and find all of the files that contain bf.  \n",
    "\n",
    "Luckily none of my files have bf in the name other than the file extension. Be careful with regexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bf_files = !find {bayenv_dir} | grep bf\n",
    "len(bf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bf_files[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bf_data = {}\n",
    "for b in bf_files:\n",
    "    d = open(b).readlines()\n",
    "    d = d[-1].strip().split(\"\\t\")[1:]\n",
    "    if len(d) == 12:\n",
    "        bf_data[os.path.basename(b).replace(\".txt.bf\",\"\")] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bf = pd.DataFrame(bf_data).T.astype(float)\n",
    "bf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(bf.ix[:,1], bf.ix[:,2])\n",
    "plt.xlabel(\"Spearman\")\n",
    "plt.ylabel(\"Pearson\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(bf.ix[:,1], bf.ix[:,0])\n",
    "plt.xlabel(\"Spearman\")\n",
    "plt.ylabel(\"Bayes factor\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Outlier analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_outliers(df, key, num_std):\n",
    "    if key == \"bf\":\n",
    "        key = 0\n",
    "    elif key == \"rho\":\n",
    "        key = 1  \n",
    "    outliers = {}   \n",
    "    ai = 0\n",
    "    for i in xrange(key, len(df.columns), 3):\n",
    "        d = df.ix[:,i]\n",
    "        d_std = np.std(d)\n",
    "        d_mean = np.mean(d)\n",
    "        cutoffs = [d_mean + (num_std*d_std), d_mean - (num_std*d_std)]\n",
    "        env = ai_cols[ai]\n",
    "        outliers[env] = d[(d >= cutoffs[0]) | (d <= cutoffs[1])]\n",
    "        ai += 1\n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_outliers(df, key, num_std):\n",
    "    if key == \"bf\":\n",
    "        key = 0\n",
    "    elif key == \"rho\":\n",
    "        key = 1   \n",
    "    ai = 0\n",
    "    for i in xrange(key, len(df.columns), 3):\n",
    "        d = df.ix[:,i]\n",
    "        d_std = np.std(d)\n",
    "        d_mean = np.mean(d)\n",
    "        env = ai_cols[ai]\n",
    "        ax = plt.gca()\n",
    "        if key == 0:\n",
    "            ax.set_yscale('log')\n",
    "        plt.hist(d, bins=100)\n",
    "        plt.xlim(np.min(d), d_mean+(num_std*d_std))\n",
    "        plt.title(\"%s $\\mu = %.4f \\pm %.4f [%.4f, %.4f])$\" % (env,\n",
    "                                                            d_mean,\n",
    "                                                            d_std,\n",
    "                                                            np.min(d),\n",
    "                                                            np.max(d)))\n",
    "        plt.show()\n",
    "        ai += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_outliers(bf, \"bf\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_outliers(bf, \"rho\", 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bf_outliers = get_outliers(bf, \"bf\", 6)    \n",
    "rho_outliers = get_outliers(bf, \"rho\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "library(VennDiagram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_venn(outliers, title):\n",
    "    keys = sorted(list(outliers.keys()))\n",
    "    a1 = set(outliers[keys[0]].index)\n",
    "    a2 = set(outliers[keys[1]].index)\n",
    "    a3 = set(outliers[keys[2]].index)\n",
    "    a4 = set(outliers[keys[3]].index)\n",
    "    area1 = len(a1)\n",
    "    area2 = len(a2)\n",
    "    area3 = len(a3) \n",
    "    area4 = len(a4)\n",
    "    n12 = len(a1.intersection(a2))\n",
    "    n13 = len(a1.intersection(a3))\n",
    "    n14 = len(a1.intersection(a4))\n",
    "    n23 = len(a2.intersection(a3))\n",
    "    n24 = len(a2.intersection(a4))\n",
    "    n34 = len(a3.intersection(a4))\n",
    "    n123 = len(set.intersection(a1, a2, a3))\n",
    "    n124 = len(set.intersection(a1, a2, a4))\n",
    "    n134 = len(set.intersection(a1, a3, a4))\n",
    "    n234 = len(set.intersection(a2, a3, a4))\n",
    "    n1234 = len(set.intersection(a1, a2, a3, a4))\n",
    "    venn = \"venn_%s.png\" % title.replace(\" \", \"_\")\n",
    "    r(\"library(VennDiagram)\")\n",
    "    r(\"png('%s')\" % venn)\n",
    "    r('draw.quad.venn')(area1, \n",
    "                  area2,\n",
    "                  area3,\n",
    "                  area4,\n",
    "                  n12,\n",
    "                  n13,\n",
    "                  n14,\n",
    "                  n23,\n",
    "                  n24,\n",
    "                  n34,\n",
    "                  n123,\n",
    "                  n124,\n",
    "                  n134,\n",
    "                  n234,\n",
    "                  n1234,\n",
    "                       category=keys)\n",
    "    r('dev.off()')\n",
    "    return venn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(draw_venn(bf_outliers, \"Bayes factor outliers\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(draw_venn(rho_outliers, \"Rho outliers\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_outliers = {}\n",
    "for key in bf_outliers:\n",
    "    a = bf_outliers[key].index\n",
    "    b = rho_outliers[key].index\n",
    "    combined_outliers[key] = pd.Series(index=a.intersection(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(draw_venn(combined_outliers, \"combined\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boxplot_data = {}\n",
    "for key, val in bf_outliers.items():\n",
    "    val = val.sort(inplace=False, ascending=False)\n",
    "    boxplot_data[key] = {val.index[0]: val[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boxplot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#bayenv_df_ai_basegt = bayenv_df_ai.apply(convert_to_snpassoc)\n",
    "for env in boxplot_data:\n",
    "    for snp in boxplot_data[env]:\n",
    "        vals = {}\n",
    "        for gt, group in bayenv_df_ai.groupby(snp):\n",
    "            if not gt == 'NA':\n",
    "                vals[gt.replace(\"/\", \"\")] = group[env]\n",
    "        vals = pd.DataFrame(vals, dtype=float)\n",
    "        vals.index.name = env\n",
    "\n",
    "        sns.boxplot([vals[x].dropna() for x in vals], \n",
    "                    names=vals.columns)\n",
    "        plt.title(\"%s/%s (%.4f)\" % (snp, vals.index.name, boxplot_data[env][snp]))\n",
    "        plt.show()\n",
    "\n",
    "        sns.violinplot([vals[x].dropna() for x in vals], \n",
    "                    names=vals.columns)\n",
    "        plt.title(\"%s/%s (%.4f)\" % (snp, vals.index.name, boxplot_data[env][snp]))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "perloc = hdf.get(\"perloc\").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xtx_files = !find {bayenv_dir} | grep xtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xtx_data = {}\n",
    "for f in xtx_files:\n",
    "    f = open(f).readlines()[-1].split()\n",
    "    f[0] = f[0].replace(\".txt\", \"\")\n",
    "    xtx_data[f[0]] = float(f[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xtx = pd.Series(xtx_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xtx_perloc = pd.concat((xtx, perloc), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = [\"xtx\"]\n",
    "cols.extend(xtx_perloc.columns[1:])\n",
    "xtx_perloc.columns = cols\n",
    "xtx_perloc = xtx_perloc.dropna()\n",
    "xtx_plot_data = xtx_perloc[xtx_perloc.Fst < 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(xtx_plot_data.xtx, xtx_plot_data.Fst)\n",
    "plt.title(\"$X^{T}\\!X$ vs. $F_{ST}$ for $n = %d$ loci\" % (len(xtx_plot_data)))\n",
    "plt.xlabel(\"$X^T\\!X$\")\n",
    "plt.ylabel(\"$F_{ST}$\")\n",
    "m, b, r, p, se = sp.stats.linregress(xtx_plot_data.xtx.values, xtx_plot_data.Fst.values)\n",
    "plt.plot(xtx_plot_data.xtx, (xtx_plot_data.xtx*m + b), c=\"r\", lw=1)\n",
    "plt.text(11, -0.075, \"$y = %.4fx %.4f \\ (r = %.4f, p = %.4f)$\" % (m, b, r, p), fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}