{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Exploring Patterns of Population Structure and Environmental Associations to Aridity Across the Range of Loblolly Pine\n",
    "\n",
    "##Introduction\n",
    "\n",
    "In this set of analyses, we will be making use of data from the Eckert et al. 2010 paper to explore patterns of phenotypic and environmental associations among populations of loblolly pine.\n",
    "\n",
    "\n",
    "###Abstract\n",
    "\n",
    "Natural populations of forest trees exhibit striking phenotypic adaptations to diverse environmental\n",
    "gradients, thereby making them appealing subjects for the study of genes underlying ecologically relevant phenotypes. Here, we use a genome-wide data set of single nucleotide polymorphisms genotyped across 3059 functional genes to study patterns of population structure and identify loci associated with aridity across the natural range of loblolly pine (Pinus taeda L.). Overall patterns of population structure, as inferred using principal components and Bayesian cluster analyses, were consistent with three genetic clusters likely resulting from expansions out of Pleistocene refugia located in Mexico and Florida. A novel application of association analysis, which removes the confounding effects of shared ancestry on correlations between genetic and environmental variation, identified five loci correlated with aridity. These loci were primarily involved with abiotic stress response to temperature and drought. A unique set of 24 loci was identified as FST outliers on the basis of the genetic clusters identified previously and after accounting for expansions out of Pleistocene refugia. These loci were involved with a diversity of physiological processes. Identification of nonoverlapping sets of loci highlights the fundamental differences implicit in the use of either method and suggests a pluralistic, yet complementary, approach to the identification of genes underlying ecologically relevant phenotypes.\n",
    "\n",
    "\n",
    "##Overview of tasks\n",
    "\n",
    "In general, what you will be doing is working your way from loading and saving data related to this study, to corrections for population structure, to looking for associations between genotypes and phenotypes, genotypes and the environment (`Bayenv2`), and genotypes+phenotypes+environment (`SQUAT`)\n",
    "\n",
    "## This notebook\n",
    "\n",
    "With this notebook, you will explore both environmental and phenotypic associations with your SNP loci using the method we call SQUAT or Berg/Coop.  The citation for this method is in the slides, but you can get the paper [here](http://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1004412).\n",
    "\n",
    "In this notebook you will work with:\n",
    "\n",
    "1. population specific allele frequencies\n",
    "1. SNPassoc in R to obtain genotypic mean estimates for your 3 genotypic classes\n",
    "1. Calclulate $\\alpha$ for each of your SNPs\n",
    "1. Use estimates of frequency and alpha to compute $Qx$\n",
    "1. Investigate effects of populations using $z$-scores\n",
    "\n",
    "As with the previous notebook, execute the cell with the imports and continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from IPython.display import Image\n",
    "import pandas as pd\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import rpy2\n",
    "from rpy2 import robjects as ro\n",
    "import pandas.rpy.common as com\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import operator\n",
    "import scipy as sp\n",
    "import traceback\n",
    "from sklearn import preprocessing\n",
    "from IPython.parallel import Client\n",
    "from subprocess import Popen, PIPE\n",
    "import shutil\n",
    "from IPython.display import FileLink, FileLinks, Image\n",
    "import psutil\n",
    "import multiprocessing\n",
    "from hdfstorehelper import HDFStoreHelper\n",
    "import warnings\n",
    "import pandas\n",
    "import dill\n",
    "import statsmodels as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy.stats.stats import pearsonr\n",
    "warnings.simplefilter(\"ignore\", pandas.io.pytables.PerformanceWarning)\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext rpy2.ipython\n",
    "pd.set_option('display.width', 80)\n",
    "pd.set_option('max.columns', 30)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "library(qvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert_to_snpassoc(col):\n",
    "    if \"-\" in col.name:\n",
    "        freqs = af[col.name]\n",
    "        trans = {11: \"%s/%s\" % (freqs[\"A\"], freqs[\"A\"]),\n",
    "                12: \"%s/%s\" % (freqs[\"A\"], freqs[\"a\"]),\n",
    "                22: \"%s/%s\" % (freqs[\"a\"], freqs[\"a\"]),\n",
    "                \"NA\":\"NA\"}\n",
    "        return col.apply(lambda x: trans[x])\n",
    "    return col\n",
    "\n",
    "def get_phenotype(row):\n",
    "    return np.max(pheno[(pheno.Longitude==row.long) & (pheno.Latitude==row.lat)])\n",
    "\n",
    "def center_and_standardize_value(val, u, var):\n",
    "    if val == -1:\n",
    "        return 0.0\n",
    "    return (val-u)/np.sqrt(var)\n",
    "\n",
    "def center_and_standardize(snp):\n",
    "    maf = af.ix[\"q\",snp.name]\n",
    "    u = np.mean([x for x in snp if x != -1])\n",
    "    var = np.sqrt(maf*(1-maf))\n",
    "    return snp.apply(center_and_standardize_value, args=(u, var))\n",
    "\n",
    "def add_county_id(row):\n",
    "    key = \"%s_%s\" % (row.county,row.state)\n",
    "    if key in county_id:\n",
    "        return county_id[key]\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = ro.r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Let's pull in some data\n",
    "\n",
    "You'll need:\n",
    "\n",
    "* your trait of interest\n",
    "* the dictionary mapping population names to ids\n",
    "* the PCA covariance matrix esimtated earlier\n",
    "* your phenotypes\n",
    "* the global allele frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdf = HDFStoreHelper(\"data.hd5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trait_name = dill.load(open(\"trait_name.dill\"))\n",
    "county_id = dill.load(open(\"county_id.dill\"))\n",
    "pca_cov = hdf.get('pca_cov')\n",
    "loc_hierf = hdf.get(\"loc_hierf\")\n",
    "pheno = hdf.get('pheno')\n",
    "af = hdf.get(\"af\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trait = loc_hierf.apply(get_phenotype, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trait_loc_hierf = trait.join(loc_hierf, how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trait_complete = trait_loc_hierf.drop(trait_loc_hierf[np.isnan(trait_loc_hierf[trait_name])].index)\n",
    "trait_complete['countyid'] = trait_complete.apply(add_county_id, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print r[\"trait_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ro.globalenv['trait_data'] = trait_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trait_complete.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Let's keep only those rows that have the trait we care about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trait_complete = trait_complete[trait_complete.countyid > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trait_complete.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Let's convert to SNPassoc format and join the data with the PCA covariance matrix\n",
    "\n",
    "This gets all of the data together in one frame so that we can use the PCs to correct for population structure in our genotypic mean estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trait_snpassoc = trait_complete.apply(convert_to_snpassoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trait_snpassoc_pca = trait_snpassoc.join(pca_cov, how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trait_snpassoc_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Let's make the data more manageable.\n",
    "\n",
    "Have a look at it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trait_snpassoc_pca = trait_snpassoc_pca.drop(['county_state',\n",
    "                         'Longitude',\n",
    "                         'Latitude',\n",
    "                         'county',\n",
    "                         'state',\n",
    "                         'lat',\n",
    "                         'long',\n",
    "                         'countyid'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trait_snpassoc_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####This part gets a little crazy, mostly due to how we chose to run SNPassoc in the past.  Making an R script seems to be the most straight-forward way to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trait_snpassoc_pca.to_csv(\"%s_snpassoc.txt\" % trait_name,\n",
    "                             header=True,\n",
    "                             index=True,\n",
    "                             sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"%s_snpassoc.txt\" % trait_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_snpassoc_file(df, input_file, num_pca_axes):\n",
    "    pheno = df.columns[0:1]\n",
    "    out_files = []\n",
    "    for p in pheno:\n",
    "        with open(\"snpassoc_%s_%s.R\" % (os.path.basename(input_file), p.lower()), \"w\") as o:\n",
    "            print \"writing %s\" % o.name\n",
    "            out_files.append(o.name)\n",
    "            text = '''\n",
    "library(SNPassoc)\n",
    "\n",
    "d = read.table('%s', sep=\"\\\\t\", row.names=1, header=T)\n",
    "\n",
    "#subtract b/c those are the PCA axes\n",
    "snp_cols = 2:(ncol(d)-%d)\n",
    "snp_data = setupSNP(d, colSNPs=snp_cols, sep=\"/\")\n",
    "pca_cols = (ncol(d)-%d):ncol(d)\n",
    "pca_data = d[,pca_cols]\n",
    "\n",
    "wg = WGassociation(%s~1+pca_data$PC1+pca_data$PC2+pca_data$PC3+pca_data$PC4+\n",
    "pca_data$PC5+pca_data$PC6+pca_data$PC7+pca_data$PC8+pca_data$PC9+pca_data$PC10+\n",
    "+pca_data$PC11+pca_data$PC12+pca_data$PC13+pca_data$PC14, \n",
    "data=snp_data, \n",
    "model=\"co\", \n",
    "genotypingRate=5)\n",
    "\n",
    "saveRDS(wg, \"wg_%s_co.rds\")\n",
    "stats = WGstats(wg)\n",
    "saveRDS(stats, \"wgstats_%s.rds\")\n",
    "''' % (input_file, \n",
    "       num_pca_axes,\n",
    "       num_pca_axes-1,\n",
    "       p, \n",
    "       p.lower(), \n",
    "       p.lower())\n",
    "        \n",
    "            o.write(text)\n",
    "    return out_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_snpassoc_file(trait_snpassoc_pca, \"%s_snpassoc.txt\" % trait_name, 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Run in R\n",
    "\n",
    "```R\n",
    "source('snpassoc_cfried_melezitose_snpassoc.txt_melezitose.R')\n",
    "```\n",
    "\n",
    "If you've used a different file name, just change it so that it's the file that you get back from the function above.  You can do this from the terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Let's pull that snpassoc data.\n",
    "\n",
    "The file names may change depending on your trait.  Just get the names of those files and change in them for `wg_trait_co.rds` and `wgstats_trait.rds`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "wg_trait_co.rds = readRDS('wg_melezitose_co.rds')\n",
    "wgstats_trait.rds = readRDS('wgstats_melezitose.rds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####This pulls the data from R into python so we can do something useful with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wgstats_trait = r['wgstats_trait.rds']\n",
    "wgstats_trait_labels = r('labels(wg_trait_co.rds)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wgstats = {trait_name:[wgstats_trait, wgstats_trait_labels.rx2(1)]}\n",
    "for key, datalist in wgstats.items():\n",
    "    print \"converting %s\" % key\n",
    "    wgstats[key] = [com.convert_robj(x) for x in datalist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####This set of functions computes $\\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_alleles(data):\n",
    "    a = set()\n",
    "    for x in data.index:\n",
    "        for elem in x.split(\"/\"):\n",
    "            a.add(elem)\n",
    "    return list(a)  \n",
    "\n",
    "def get_allele_freqs_wg(data, AA, Aa, aa):\n",
    "    total = np.sum(data['n'])*2\n",
    "    A = data.ix[AA, \"n\"]*2 + data.ix[Aa, \"n\"]\n",
    "    a = data.ix[aa, \"n\"]*2 + data.ix[Aa, \"n\"]\n",
    "    return A/total, a/total\n",
    "\n",
    "def get_genotypes(data, alleles):\n",
    "    homos = [\"%s/%s\" % (x,x) for x in alleles]\n",
    "    Aa = \"%s/%s\" % (alleles[0], alleles[1])\n",
    "    if Aa not in data.index:\n",
    "        Aa = Aa[::-1] #reverse it\n",
    "    AA, aa = homos\n",
    "    if data.ix[AA, \"n\"] < data.ix[aa, \"n\"]:\n",
    "        AA, aa = homos[::-1] #reverse it so that major is first\n",
    "    return AA, Aa, aa\n",
    "\n",
    "def get_genotypic_values(data, alleles):\n",
    "    AA, Aa, aa = get_genotypes(data, alleles)\n",
    "    G_AA = float(data.ix[AA, 'me'])\n",
    "    G_aa = float(data.ix[aa, 'me'])\n",
    "    additive = (G_AA-G_aa)/2\n",
    "    G_Aa = float(data.ix[Aa, 'me'])\n",
    "    dominance = G_Aa - ((G_AA+G_aa)/2)\n",
    "    return additive, dominance, AA, Aa, aa\n",
    "    \n",
    "def get_alpha(data):\n",
    "    alleles = get_alleles(data)\n",
    "    additive, dominance, AA, Aa, aa = get_genotypic_values(data, alleles)\n",
    "    p, q = get_allele_freqs_wg(data, AA, Aa, aa)\n",
    "    alpha = additive + (dominance*(q-p))\n",
    "    return alpha, AA, aa, p, q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Compute alpha values for your phenotype\n",
    "\n",
    "The code is from a larger analysis that I've done, and I didn't change it for here.  The `python for p in wgstats:` part is for if you run multiple phenotypes at the same time.  For these examples, though, you just have one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha_vals = {}\n",
    "for p in wgstats:\n",
    "    print \"running %s\" % p\n",
    "    df = pd.DataFrame(index=[\"alpha\", \"p-value\", \"AA\", \"aa\", \"p\", \"q\"])\n",
    "    alpha_vals[p] = df\n",
    "    d = wgstats[p][0]\n",
    "    labels = wgstats[p][1]\n",
    "    for i, locus in enumerate(d):\n",
    "        try:\n",
    "            data = pd.DataFrame(d[locus])\n",
    "            snp = labels[i]\n",
    "            genotypes = [g for g in data.index if \"/\" in g]\n",
    "            data = data.ix[genotypes,:]\n",
    "            pvalue = data['p-value'].dropna()[0]\n",
    "            if len(genotypes) == 3:\n",
    "                alpha, AA, aa, p, q = get_alpha(data)\n",
    "                df[snp] = [alpha, pvalue, AA, aa, p, q]\n",
    "        except Exception as e: \n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####The alpha vals `DataFrame` actually contains more than just $\\alpha$\n",
    "\n",
    "I tend to do this a lot, group stats together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alpha_vals[trait_name].head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Have a look at the p-values.  Does the distribution surprise you?  \n",
    "\n",
    "Also do the same for the $\\alpha$ values.  Ask your self, or a neighbor, the same question.  \n",
    "\n",
    "* What does this tell you about the effect sizes of the SNPs?  \n",
    "* Is it what you would expect for polygenic adaptation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(alpha_vals[trait_name].ix['p-value',:], bins=30)\n",
    "plt.title(\"p-values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(alpha_vals[trait_name].ix['alpha',:], bins=30)\n",
    "plt.title(\"alpha values $\\mu %.4f \\pm %.4f \\ [%.4f, %.4f]$\" % (np.mean(alpha_vals[trait_name].ix['alpha',:]),\n",
    "                                                            np.std(alpha_vals[trait_name].ix['alpha',:]),\n",
    "                                                            np.min(alpha_vals[trait_name].ix['alpha',:]),\n",
    "                                                             np.max(alpha_vals[trait_name].ix['alpha',:])))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_homozygous(gt):\n",
    "    if len(set([x.strip() for x in gt.split(\"/\")])) == 1:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def get_allele_counts(counts):\n",
    "    a = {}\n",
    "    het = 0\n",
    "    for gt in counts.index:\n",
    "        for allele in [x.strip() for x in gt.split(\"/\")]:\n",
    "            if not allele in a:\n",
    "                a[allele] = 0\n",
    "            a[allele] += counts[gt]\n",
    "        if not is_homozygous(gt):\n",
    "            het += counts[gt]\n",
    "    return sorted(a.items(), key=lambda x: x[1], reverse=True), het\n",
    "\n",
    "def get_correction(n):\n",
    "    #for finite sample size\n",
    "    return (2*n)/(2*n-1)\n",
    "\n",
    "def get_allele_freqs(locus):\n",
    "    locus = locus[locus != '?/?']\n",
    "    locus = locus[locus != 'NA']\n",
    "    c = locus.value_counts()\n",
    "    c = c.sort(inplace=False, ascending=False)\n",
    "    allele_counts = get_allele_counts(c)\n",
    "    total_alleles = 2.0*sum(c)\n",
    "    num_individuals = sum(c)\n",
    "    A = \"\"\n",
    "    a = \"\"\n",
    "    P = 0\n",
    "    Q = 0\n",
    "    if len(allele_counts[0]) == 2:\n",
    "        A = allele_counts[0][0][0]\n",
    "        a = allele_counts[0][1][0]\n",
    "        P = allele_counts[0][0][1]\n",
    "        Q = allele_counts[0][1][1]\n",
    "    else:\n",
    "        A = allele_counts[0][0][0]\n",
    "        P = P = allele_counts[0][0][1]\n",
    "    PQ = allele_counts[-1]\n",
    "    p = P/total_alleles\n",
    "    q = Q/total_alleles\n",
    "    assert p + q == 1.0\n",
    "    He = 2 * p * q * get_correction(num_individuals)\n",
    "    Ho = PQ*1.0/num_individuals\n",
    "    Fis = 1 - (Ho/He)\n",
    "    #print p, q, He, Ho, Fis\n",
    "    ret = pd.Series({\"p\":p, \n",
    "                      \"q\":q,\n",
    "                      \"P\":P,\n",
    "                      \"Q\":Q,\n",
    "                      \"He\":He,\n",
    "                      \"Ho\":Ho, \n",
    "                      \"Fis\":Fis,\n",
    "                    \"PQ\": PQ,\n",
    "                    \"total_alleles\":total_alleles,\n",
    "                    \"num_indiv\":num_individuals,\n",
    "                    \"A\":A,\n",
    "                    \"a\":a})\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####We need to organize our data by county again, find those which have counties, and then recompute the allele frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trait_snpassoc_pca_county = pd.concat([trait_complete.countyid, trait_snpassoc_pca], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trait_snpassoc_pca_county = trait_snpassoc_pca_county.drop(trait_snpassoc_pca_county[np.isnan(trait_snpassoc_pca_county[trait_name])].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snpassoc_af = trait_snpassoc_pca_county.ix[:,2:-14].apply(get_allele_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snpassoc_af"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Additionally, now that we have counties, we can compute allele frequencies for each county.\n",
    "\n",
    "This takes a minute or two, there are 34 counties right now.  Be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pop_allele_freqs = {}\n",
    "for pop,data in trait_snpassoc_pca_county.groupby(\"countyid\"):\n",
    "    print \"getting allele freqs for pop % d\" % pop\n",
    "    pop_allele_freqs[pop] = data.ix[:,2:-14].apply(get_allele_freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####We only want to include populations with big enough samples.  Remember doing this earlier?  Let's do it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_usable_counties(county):\n",
    "    if county.county_state in county_id:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "data_ai = hdf.get(\"data_ai\")\n",
    "data_ai['usable'] = data_ai.apply(get_usable_counties, axis=1)\n",
    "data_ai = data_ai.drop(data_ai[data_ai.usable == False].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####This section below is a nightmare\n",
    "\n",
    "Reason \\#7643 why statisticians should not write software.  The SQUAT method is a compendium of many files and directories that all must play nicely together.  Most of the work has been done (I think) to make this as seemless as possible.  Fingers crossed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_gwas_data_file(df, pheno, outdir):\n",
    "    out = \"%s_gwas_data_file.txt\" % pheno\n",
    "    out = os.path.join(outdir, out)\n",
    "    df = df.sort_index()\n",
    "    df[['A1', 'A2', 'EFF', 'FRQ']].to_csv(out,\n",
    "                                          header=True, \n",
    "                                          index=True,\n",
    "                                          sep=\"\\t\")\n",
    "    print out\n",
    "    return out\n",
    "\n",
    "def write_freqs_file(df, pheno, pop_freqs, outdir):\n",
    "    out = \"%s_freqs_file.txt\" % pheno\n",
    "    out = os.path.join(outdir, out)\n",
    "    print out\n",
    "    with open(out, \"w\") as o:\n",
    "        o.write(\"SNP\\tCLST\\tA1\\tA2\\tFRQ\\n\")\n",
    "        for pop, data in pop_freqs.items():\n",
    "            m = data.T.merge(df, how=\"inner\", left_index=True, right_index=True)\n",
    "            m['population'] = pop\n",
    "            m.index.name = 'SNP'\n",
    "            m = m.sort_index()\n",
    "            o.write(m[['population','A1','A2','p']].to_csv(header=False, \n",
    "                                                             index=True,\n",
    "                                                             sep=\"\\t\"))\n",
    "def write_match_pop_file(df, pheno, pop_freqs, pop, outdir):\n",
    "    out = \"%s_match_pop_file.txt\" % pheno\n",
    "    out = os.path.join(outdir, out)\n",
    "    print out\n",
    "    with open(out, \"w\") as o:\n",
    "        o.write(\"SNP\\tCLST\\tA1\\tA2\\tFRQ\\n\")\n",
    "        for key, data in pop_freqs.items():\n",
    "            if key == pop:\n",
    "                m = data.T.merge(df, how=\"inner\", left_index=True, right_index=True)\n",
    "                m['population'] = pop\n",
    "                m.index.name = 'SNP'\n",
    "                m = m.sort_index()\n",
    "                o.write(m[['population','A1','A2','p']].to_csv(header=False, \n",
    "                                                                 index=True,\n",
    "                                                                 sep=\"\\t\"))\n",
    "                break\n",
    "                \n",
    "def write_full_dataset_file(df, pheno, pop_freqs, outdir):\n",
    "    out = \"%s_full_dataset_file.txt\" % pheno\n",
    "    out = os.path.join(outdir, out)\n",
    "    print out\n",
    "    with open(out, \"w\") as o:\n",
    "        o.write(\"SNP\\tCLST\\tA1\\tA2\\tFRQ\\n\")\n",
    "        for pop, data in pop_freqs.items():\n",
    "            m = data.T.merge(df, how=\"inner\", left_index=True, right_index=True)\n",
    "            m['population'] = pop\n",
    "            m.index.name = 'SNP'\n",
    "            m = m.sort_index()\n",
    "            o.write(m[['population','A1','A2','p']].to_csv(header=False, \n",
    "                                                             index=True,\n",
    "                                                             sep=\"\\t\"))   \n",
    "def write_env_var_data_file(pheno, pop_freqs, outdir):\n",
    "    cols = [x for x in data_ai.columns if \"AI\" in x]\n",
    "    for c in cols:\n",
    "        pop_id = 1\n",
    "        out = \"%s_%s_env_var_data_file.txt\" % (c, pheno)\n",
    "        out = os.path.join(outdir, out)\n",
    "        print out\n",
    "        with open(out, \"w\") as o:\n",
    "            o.write(\"CLST\\tENV\\tREG\\n\")\n",
    "            for pop, d in data_ai.groupby('county_state'):\n",
    "                pop = float(county_id[pop])*1.0\n",
    "                if pop in pop_freqs:\n",
    "                    o.write(\"%.1f\\t%f\\t%.1f\\n\" % (pop, d[c], pop_id))\n",
    "                    pop_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_qvalues(pvalues):\n",
    "    qvalue = r(\"qvalue\")\n",
    "    pvalues = r(\"as.numeric\")(pvalues)\n",
    "    qobj = qvalue(pvalues)\n",
    "    qvalues = qobj.rx2(\"qvalues\")\n",
    "    return np.array(qvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####This is a critial section\n",
    "\n",
    "We must make a decision on what SNPs to include as GWAS snps (those SNPs we think might be important).  The better we do here at choosing, the better our results may end up.  This is tricky, too few and you won't find anything.  Too many and the model might get messed up.  Until more people start using this, it's hard to know what to do.  In our case, we take all \n",
    "SNPs with raw $p$-values < 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "squat_outdir = \"squat\" #change for your username\n",
    "if not os.path.exists(squat_outdir):\n",
    "    os.mkdir(squat_outdir)\n",
    "\n",
    "for p in alpha_vals:\n",
    "    full = alpha_vals[p].T\n",
    "    full['q-value'] = get_qvalues(full['p-value'])\n",
    "    full.index = [x.replace(\".\", \"-\") for x in full.index]\n",
    "    full.index = [x[1:] if x.startswith(\"X\") else x for x in full.index]\n",
    "    full.index.name = \"SNP\"\n",
    "    full.AA = full.AA.apply(lambda x: x[0])\n",
    "    full.aa = full.aa.apply(lambda x: x[0])\n",
    "    full = full.rename(columns={'alpha':'EFF',\n",
    "                                'AA':'A1',\n",
    "                                'aa':'A2',\n",
    "                                'p': 'FRQ'})\n",
    "    candidates = full[full['p-value']<0.05]\n",
    "    plt.hist(full['q-value'], bins=100)\n",
    "    plt.title(\"q-value\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.hist(full['p-value'], bins=100)\n",
    "    plt.title(\"p-value\")\n",
    "    plt.show()\n",
    "    print \"chose %d candidates\" % len(candidates)\n",
    "    write_gwas_data_file(candidates, p, squat_outdir)\n",
    "    write_freqs_file(candidates, p, pop_allele_freqs, squat_outdir)\n",
    "    write_match_pop_file(full, p, pop_allele_freqs, 2, squat_outdir)\n",
    "    write_full_dataset_file(full, p, pop_allele_freqs, squat_outdir)\n",
    "    write_env_var_data_file(p, pop_allele_freqs, squat_outdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Examine the output above.\n",
    "\n",
    "We've written several files.  \n",
    "\n",
    "1. Can you decifer what each file is for?  Have a look at them with either `!head` or `!cat`. \n",
    "\n",
    "1. How many candidate genes are in your GWAS?\n",
    "\n",
    "1. Why might there be 4 files for AI?\n",
    "\n",
    "The following bits of code set up the input file for SQUAT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env_squat_files = !find {squat_outdir} | grep {trait_name} | grep env_var | grep AI\n",
    "print env_squat_files\n",
    "env_squat_files = [os.path.basename(x) for x in env_squat_files]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env_var_file_string = \"list(%s)\" % \", \".join([\"'%s'\" % x for x in env_squat_files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "squat_scripts_dir = \"/gdc_home4/cfried/src/PolygenicAdaptationCode/Scripts\"\n",
    "!rm {squat_outdir}/Scripts\n",
    "!cd {squat_outdir} && ln -s {squat_scripts_dir} .\n",
    "def get_squat_vars(pheno):\n",
    "    d = {\"gwas.data.file\":\"'%s_gwas_data_file.txt'\" % pheno,\n",
    "         \"freqs.file\":\"'%s_freqs_file.txt'\" % pheno,\n",
    "         \"env.var.data.files\": env_var_file_string,\n",
    "         \"match.pop.file\":\"'%s_match_pop_file.txt'\" % pheno,\n",
    "         \"full.dataset.file\":\"'%s_full_dataset_file.txt'\" % pheno,\n",
    "         \"path\":\"'%s'\" % pheno,\n",
    "         \"match.categories\":\"c('MAF')\",\n",
    "         \"match.bins\":\"list(seq(0,0.5,0.02), c(2), seq(0,1000,100))\",\n",
    "         \"cov.SNPs.per.cycle\":5000,\n",
    "         \"cov.cycles\":1,\n",
    "         \"null.phenos.per.cycle\":1000,\n",
    "         \"null.cycles\":1,\n",
    "         \"load.cov.mat\":\"F\",\n",
    "         \"sim.null\":\"T\",\n",
    "         \"check.allele.orientation\":\"F\"}\n",
    "    return ',\\n'.join(\"%s=%s\" % (key,val) for (key,val) in d.items())\n",
    "\n",
    "def create_squat_run_file(pheno):\n",
    "    squat_file = os.path.join(squat_outdir, \"squat_%s.r\" % pheno)\n",
    "    with open(squat_file, \"w\") as o:\n",
    "        o.write('system(\"rm -rf %s\")\\n'% pheno)\n",
    "        o.write(\"source('%s')\\n\" % os.path.join(squat_scripts_dir, \"CreateTraitFile.R\"))\n",
    "        o.write(\"source('%s')\\n\" % os.path.join(squat_scripts_dir, \"functions.R\"))\n",
    "        o.write(\"PolygenicAdaptationFunction(%s)\\n\" % get_squat_vars(pheno))\n",
    "    return squat_file\n",
    "\n",
    "for pheno in alpha_vals:\n",
    "    squat_file = create_squat_run_file(pheno)\n",
    "    print squat_file\n",
    "    !cat $squat_file\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####If all goes well, you should be able to run SQUAT.  If it fails, come and get me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_squat(p):\n",
    "    print \"running %s\" % p\n",
    "    output = \"%s/%s\" % (squat_outdir, p)\n",
    "    if os.path.exists(output):\n",
    "        !rm -rf {output}\n",
    "    cmds = [\"setwd('%s')\" % squat_outdir,\n",
    "            'source(\"squat_%s.r\")' % (p),\n",
    "            \"setwd('../')\"]\n",
    "    for cmd in cmds:\n",
    "        print cmd\n",
    "        r(cmd)\n",
    "    \n",
    "run_squat(trait_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####You can get the files that SQUAT wrote directly from the file system using `find` and `grep`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rfiles = !find {squat_outdir} | grep Robj | grep Output | grep {trait_name}\n",
    "bc = {}\n",
    "for f in rfiles:\n",
    "    d = f.split(\"/\")\n",
    "    if not d[1] in bc:\n",
    "        bc[d[1]] = []\n",
    "    bc[d[1]].append(f)\n",
    "bc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Let's examine the outputs\n",
    "\n",
    "1. What is the $Qx$ value.  Is it significant?\n",
    "1. Is anything interesting reported in the decomosition into $F_{ST}$ or LD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for pheno in bc:\n",
    "    print pheno\n",
    "    for obj in bc[pheno]:\n",
    "        r('load(\"%s\")' % obj)\n",
    "    print r(\"the.stats\")\n",
    "    print(\"------------------\")\n",
    "    print r(\"p.vals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####We can now look at the influence of populations in driving environmental correlations\n",
    "\n",
    "1. Are there populations which show elevated or depressed assocaition with environment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "the_stats = r['the.stats']\n",
    "the_stats = com.convert_robj(the_stats.rx(\"ind.Z\"))['ind.Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_county(elem):\n",
    "    for k, v in county_id.items():\n",
    "        if int(v) == int(elem):\n",
    "            return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.barplot(the_stats.index, the_stats.values)\n",
    "plt.ylabel(\"Z-score\")\n",
    "plt.xlabel(\"County\")\n",
    "ax = plt.gca()\n",
    "ax.set_xticklabels(map(convert_to_county, the_stats.index), rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}